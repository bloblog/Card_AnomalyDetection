{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 임포트\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"/content/drive/MyDrive/open\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 임포트\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train 데이터셋에 정상 데이터가 월등히 많기 때문에, 랜덤으로 뽑아서 정상 데이터라고 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3c9f6601-c67e-4cb3-9171-8930d7b85d27\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55465</th>\n",
       "      <td>138017</td>\n",
       "      <td>-0.688175</td>\n",
       "      <td>0.790247</td>\n",
       "      <td>1.865052</td>\n",
       "      <td>1.552573</td>\n",
       "      <td>-0.707023</td>\n",
       "      <td>0.852548</td>\n",
       "      <td>-0.407201</td>\n",
       "      <td>0.856223</td>\n",
       "      <td>0.246791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265630</td>\n",
       "      <td>-0.502401</td>\n",
       "      <td>-0.028470</td>\n",
       "      <td>-0.008494</td>\n",
       "      <td>-0.303211</td>\n",
       "      <td>-0.459676</td>\n",
       "      <td>0.087259</td>\n",
       "      <td>0.081240</td>\n",
       "      <td>-0.148955</td>\n",
       "      <td>-0.026398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99274</th>\n",
       "      <td>248075</td>\n",
       "      <td>2.052962</td>\n",
       "      <td>0.101032</td>\n",
       "      <td>-2.061370</td>\n",
       "      <td>0.139861</td>\n",
       "      <td>0.708547</td>\n",
       "      <td>-0.992524</td>\n",
       "      <td>0.637264</td>\n",
       "      <td>-0.346230</td>\n",
       "      <td>-0.076496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165321</td>\n",
       "      <td>0.519705</td>\n",
       "      <td>0.009552</td>\n",
       "      <td>0.793465</td>\n",
       "      <td>0.402602</td>\n",
       "      <td>-0.223746</td>\n",
       "      <td>-0.065282</td>\n",
       "      <td>-0.070935</td>\n",
       "      <td>-0.099211</td>\n",
       "      <td>0.811758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40390</th>\n",
       "      <td>100377</td>\n",
       "      <td>-0.687770</td>\n",
       "      <td>-0.313360</td>\n",
       "      <td>0.017895</td>\n",
       "      <td>-0.803174</td>\n",
       "      <td>-0.354524</td>\n",
       "      <td>0.049855</td>\n",
       "      <td>0.266720</td>\n",
       "      <td>0.102801</td>\n",
       "      <td>-1.036629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266734</td>\n",
       "      <td>0.817296</td>\n",
       "      <td>-0.213891</td>\n",
       "      <td>-0.724961</td>\n",
       "      <td>-0.513179</td>\n",
       "      <td>-0.085296</td>\n",
       "      <td>-0.109877</td>\n",
       "      <td>-0.241216</td>\n",
       "      <td>1.591560</td>\n",
       "      <td>-0.202035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89688</th>\n",
       "      <td>224189</td>\n",
       "      <td>-0.231057</td>\n",
       "      <td>0.449427</td>\n",
       "      <td>-0.975381</td>\n",
       "      <td>-0.299616</td>\n",
       "      <td>-0.325232</td>\n",
       "      <td>1.071681</td>\n",
       "      <td>2.682511</td>\n",
       "      <td>-0.505684</td>\n",
       "      <td>-0.065531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123350</td>\n",
       "      <td>0.086768</td>\n",
       "      <td>0.238178</td>\n",
       "      <td>0.142864</td>\n",
       "      <td>-0.904659</td>\n",
       "      <td>-0.172483</td>\n",
       "      <td>0.121292</td>\n",
       "      <td>-0.055009</td>\n",
       "      <td>5.493188</td>\n",
       "      <td>0.693077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109178</th>\n",
       "      <td>273075</td>\n",
       "      <td>-2.669792</td>\n",
       "      <td>3.188421</td>\n",
       "      <td>-0.232399</td>\n",
       "      <td>2.206211</td>\n",
       "      <td>0.551827</td>\n",
       "      <td>0.441592</td>\n",
       "      <td>0.488823</td>\n",
       "      <td>0.465867</td>\n",
       "      <td>-1.056355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024531</td>\n",
       "      <td>-0.556317</td>\n",
       "      <td>0.216456</td>\n",
       "      <td>0.656629</td>\n",
       "      <td>-0.592143</td>\n",
       "      <td>-0.621562</td>\n",
       "      <td>-1.356682</td>\n",
       "      <td>0.015637</td>\n",
       "      <td>-0.282960</td>\n",
       "      <td>0.948272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85500</th>\n",
       "      <td>213649</td>\n",
       "      <td>-0.882367</td>\n",
       "      <td>1.482644</td>\n",
       "      <td>-0.551589</td>\n",
       "      <td>-0.544237</td>\n",
       "      <td>-0.308714</td>\n",
       "      <td>-1.267765</td>\n",
       "      <td>0.262050</td>\n",
       "      <td>0.676889</td>\n",
       "      <td>-0.127473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212423</td>\n",
       "      <td>-0.649736</td>\n",
       "      <td>0.184024</td>\n",
       "      <td>0.024476</td>\n",
       "      <td>-0.407427</td>\n",
       "      <td>0.148611</td>\n",
       "      <td>0.110147</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>-0.175784</td>\n",
       "      <td>0.641984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104541</th>\n",
       "      <td>261532</td>\n",
       "      <td>-0.073304</td>\n",
       "      <td>-0.434902</td>\n",
       "      <td>1.376645</td>\n",
       "      <td>-3.328546</td>\n",
       "      <td>-0.438309</td>\n",
       "      <td>-0.792777</td>\n",
       "      <td>0.193538</td>\n",
       "      <td>-0.421106</td>\n",
       "      <td>-1.837773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109453</td>\n",
       "      <td>0.335461</td>\n",
       "      <td>-0.201058</td>\n",
       "      <td>-0.108678</td>\n",
       "      <td>-0.290593</td>\n",
       "      <td>-0.440430</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>-0.062899</td>\n",
       "      <td>-0.194229</td>\n",
       "      <td>0.885431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19700</th>\n",
       "      <td>48987</td>\n",
       "      <td>1.241935</td>\n",
       "      <td>-0.014318</td>\n",
       "      <td>0.293430</td>\n",
       "      <td>0.301022</td>\n",
       "      <td>-0.248384</td>\n",
       "      <td>-0.221737</td>\n",
       "      <td>-0.122865</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.483780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246897</td>\n",
       "      <td>-0.568377</td>\n",
       "      <td>0.052488</td>\n",
       "      <td>-0.248136</td>\n",
       "      <td>0.293454</td>\n",
       "      <td>0.498026</td>\n",
       "      <td>-0.028723</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>-0.251520</td>\n",
       "      <td>-0.479670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98124</th>\n",
       "      <td>245244</td>\n",
       "      <td>0.017787</td>\n",
       "      <td>1.243268</td>\n",
       "      <td>-1.924447</td>\n",
       "      <td>-0.602362</td>\n",
       "      <td>1.346264</td>\n",
       "      <td>-0.878289</td>\n",
       "      <td>1.007563</td>\n",
       "      <td>0.233095</td>\n",
       "      <td>-0.772426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212993</td>\n",
       "      <td>0.545728</td>\n",
       "      <td>-0.153764</td>\n",
       "      <td>0.295110</td>\n",
       "      <td>-0.398819</td>\n",
       "      <td>0.515921</td>\n",
       "      <td>-0.072576</td>\n",
       "      <td>0.053684</td>\n",
       "      <td>-0.274436</td>\n",
       "      <td>0.798529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110496</th>\n",
       "      <td>276421</td>\n",
       "      <td>-0.486576</td>\n",
       "      <td>1.601300</td>\n",
       "      <td>-0.636852</td>\n",
       "      <td>1.116623</td>\n",
       "      <td>0.596179</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.269958</td>\n",
       "      <td>0.521708</td>\n",
       "      <td>-0.542167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169431</td>\n",
       "      <td>0.676076</td>\n",
       "      <td>0.210353</td>\n",
       "      <td>0.529007</td>\n",
       "      <td>-1.148034</td>\n",
       "      <td>-0.745850</td>\n",
       "      <td>0.381706</td>\n",
       "      <td>0.317959</td>\n",
       "      <td>0.125760</td>\n",
       "      <td>0.967892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91074 rows × 31 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c9f6601-c67e-4cb3-9171-8930d7b85d27')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3c9f6601-c67e-4cb3-9171-8930d7b85d27 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3c9f6601-c67e-4cb3-9171-8930d7b85d27');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            ID        V1        V2        V3        V4        V5        V6  \\\n",
       "55465   138017 -0.688175  0.790247  1.865052  1.552573 -0.707023  0.852548   \n",
       "99274   248075  2.052962  0.101032 -2.061370  0.139861  0.708547 -0.992524   \n",
       "40390   100377 -0.687770 -0.313360  0.017895 -0.803174 -0.354524  0.049855   \n",
       "89688   224189 -0.231057  0.449427 -0.975381 -0.299616 -0.325232  1.071681   \n",
       "109178  273075 -2.669792  3.188421 -0.232399  2.206211  0.551827  0.441592   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "85500   213649 -0.882367  1.482644 -0.551589 -0.544237 -0.308714 -1.267765   \n",
       "104541  261532 -0.073304 -0.434902  1.376645 -3.328546 -0.438309 -0.792777   \n",
       "19700    48987  1.241935 -0.014318  0.293430  0.301022 -0.248384 -0.221737   \n",
       "98124   245244  0.017787  1.243268 -1.924447 -0.602362  1.346264 -0.878289   \n",
       "110496  276421 -0.486576  1.601300 -0.636852  1.116623  0.596179  0.020089   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "55465  -0.407201  0.856223  0.246791  ... -0.265630 -0.502401 -0.028470   \n",
       "99274   0.637264 -0.346230 -0.076496  ...  0.165321  0.519705  0.009552   \n",
       "40390   0.266720  0.102801 -1.036629  ...  0.266734  0.817296 -0.213891   \n",
       "89688   2.682511 -0.505684 -0.065531  ... -0.123350  0.086768  0.238178   \n",
       "109178  0.488823  0.465867 -1.056355  ...  0.024531 -0.556317  0.216456   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "85500   0.262050  0.676889 -0.127473  ... -0.212423 -0.649736  0.184024   \n",
       "104541  0.193538 -0.421106 -1.837773  ... -0.109453  0.335461 -0.201058   \n",
       "19700  -0.122865  0.004584  0.483780  ... -0.246897 -0.568377  0.052488   \n",
       "98124   1.007563  0.233095 -0.772426  ...  0.212993  0.545728 -0.153764   \n",
       "110496  0.269958  0.521708 -0.542167  ...  0.169431  0.676076  0.210353   \n",
       "\n",
       "             V24       V25       V26       V27       V28       V29       V30  \n",
       "55465  -0.008494 -0.303211 -0.459676  0.087259  0.081240 -0.148955 -0.026398  \n",
       "99274   0.793465  0.402602 -0.223746 -0.065282 -0.070935 -0.099211  0.811758  \n",
       "40390  -0.724961 -0.513179 -0.085296 -0.109877 -0.241216  1.591560 -0.202035  \n",
       "89688   0.142864 -0.904659 -0.172483  0.121292 -0.055009  5.493188  0.693077  \n",
       "109178  0.656629 -0.592143 -0.621562 -1.356682  0.015637 -0.282960  0.948272  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "85500   0.024476 -0.407427  0.148611  0.110147  0.019524 -0.175784  0.641984  \n",
       "104541 -0.108678 -0.290593 -0.440430 -0.004679 -0.062899 -0.194229  0.885431  \n",
       "19700  -0.248136  0.293454  0.498026 -0.028723  0.002950 -0.251520 -0.479670  \n",
       "98124   0.295110 -0.398819  0.515921 -0.072576  0.053684 -0.274436  0.798529  \n",
       "110496  0.529007 -1.148034 -0.745850  0.381706  0.317959  0.125760  0.967892  \n",
       "\n",
       "[91074 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 라벨링 및 전처리\n",
    "class_0_sample = train_df.sample(frac=0.8, replace=False)\n",
    "class_0_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_sample = class_0_sample.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation 데이터 정의\n",
    "val_x = val_df.drop(columns=['ID','Class'])\n",
    "val_y = val_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차항 정의\n",
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2, axis=1))\n",
    "    #return MAE\n",
    "    return np.mean(abs(v1 - v2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reparameterization trick\n",
    "# 데이터 분포를 바꿔주는 함수 \n",
    "\n",
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 정의\n",
    "\n",
    "original_dim = class_0_sample.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 15)           465         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 10)           160         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 10)           160         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 10)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 785\n",
      "Trainable params: 785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, z, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15)                165       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                480       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 645\n",
      "Trainable params: 645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "# Instantiate the decoder model:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full VAE model\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the KL loss function: \n",
    "# VAE의 비용 함수인 KL 발산\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean))\n",
    "    # compute the KL loss\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)\n",
    "    # return the average loss over all \n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    #total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 30)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 10)                785       \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 30)                645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,430\n",
      "Trainable params: 1,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 91074 samples\n",
      "Epoch 1/30\n",
      "91074/91074 [==============================] - 1s 15us/sample - loss: 75604025095053776.0000\n",
      "Epoch 2/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 65140907985733216.0000\n",
      "Epoch 3/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 51440446917333632.0000\n",
      "Epoch 4/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 684954894346256.2500\n",
      "Epoch 5/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 347693224892937.7500\n",
      "Epoch 6/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 95118855647491.3594\n",
      "Epoch 7/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 6442797216054.4688\n",
      "Epoch 8/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 2833865729037.2935\n",
      "Epoch 9/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 640835751719.3101\n",
      "Epoch 10/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 71950781399.1636\n",
      "Epoch 11/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18626458165.7718\n",
      "Epoch 12/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 10398284772.4096\n",
      "Epoch 13/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 4930316671.5608\n",
      "Epoch 14/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 1449925066.9067\n",
      "Epoch 15/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 272326023.8464\n",
      "Epoch 16/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 66579428.5501\n",
      "Epoch 17/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 4022594.9240\n",
      "Epoch 18/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 80911.8756\n",
      "Epoch 19/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 17535.5890\n",
      "Epoch 20/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 9175.7999\n",
      "Epoch 21/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 9025.5766\n",
      "Epoch 22/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8963.6717\n",
      "Epoch 23/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8933.1940\n",
      "Epoch 24/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 8910.6510\n",
      "Epoch 25/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 8887.4914\n",
      "Epoch 26/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 8876.4670\n",
      "Epoch 27/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 8866.0757\n",
      "Epoch 28/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 8856.2559\n",
      "Epoch 29/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8846.1702\n",
      "Epoch 30/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8837.2883\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "vae_model.compile(optimizer=\"rmsprop\", loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "# Finally, we train the model:\n",
    "results = vae_model.fit(class_0_sample, class_0_sample,\n",
    "                        shuffle=True,\n",
    "                        epochs=30,\n",
    "                        batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss 그래프로 모델 학습 결과 확인\n",
    "\n",
    "plt.plot(results.history['loss'])\n",
    "#plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "val_x_pred = vae_model.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg error 0.5383939170250015\n",
      "median error 0.46984225216044795\n",
      "99Q: 1.936358577213926\n",
      "setting threshold on 1.936358577213926 \n"
     ]
    }
   ],
   "source": [
    "mae_vector = get_error_term(val_x_pred, val_x, _rmse=False)\n",
    "print(f'Avg error {np.mean(mae_vector)}\\nmedian error {np.median(mae_vector)}\\n99Q: {np.quantile(mae_vector, 0.99)}')\n",
    "print(f'setting threshold on { np.quantile(mae_vector, 0.99)} ')\n",
    "\n",
    "error_thresh = np.quantile(mae_vector, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010013351134846462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prediction한 것으로 이상치 찾아내기 \n",
    "anomalies = (mae_vector > error_thresh)\n",
    "np.count_nonzero(anomalies) / len(anomalies) #이상치 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     28432\n",
      "           1       0.07      0.63      0.12        30\n",
      "\n",
      "    accuracy                           0.99     28462\n",
      "   macro avg       0.53      0.81      0.56     28462\n",
      "weighted avg       1.00      0.99      0.99     28462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 정확도 행렬 확인 \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(val_y, anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE scaling (minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f5b09122-4da0-44e6-b6c9-b7ee797b0535\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109831</th>\n",
       "      <td>274720</td>\n",
       "      <td>-0.395676</td>\n",
       "      <td>0.777782</td>\n",
       "      <td>0.165311</td>\n",
       "      <td>-0.851634</td>\n",
       "      <td>1.117452</td>\n",
       "      <td>-1.398650</td>\n",
       "      <td>1.710514</td>\n",
       "      <td>-0.852957</td>\n",
       "      <td>0.258354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136380</td>\n",
       "      <td>0.842693</td>\n",
       "      <td>-0.305138</td>\n",
       "      <td>-0.021773</td>\n",
       "      <td>0.377381</td>\n",
       "      <td>-0.281479</td>\n",
       "      <td>-0.511858</td>\n",
       "      <td>-0.432950</td>\n",
       "      <td>0.154964</td>\n",
       "      <td>0.957272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>537</td>\n",
       "      <td>1.131541</td>\n",
       "      <td>-0.174361</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.600032</td>\n",
       "      <td>-1.021934</td>\n",
       "      <td>-0.644779</td>\n",
       "      <td>-0.428619</td>\n",
       "      <td>-0.032071</td>\n",
       "      <td>0.516318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>-0.031149</td>\n",
       "      <td>0.022845</td>\n",
       "      <td>0.414591</td>\n",
       "      <td>0.129935</td>\n",
       "      <td>0.387228</td>\n",
       "      <td>-0.016423</td>\n",
       "      <td>0.034255</td>\n",
       "      <td>0.424789</td>\n",
       "      <td>-0.990249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32744</th>\n",
       "      <td>81491</td>\n",
       "      <td>1.238043</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>-0.331998</td>\n",
       "      <td>1.345465</td>\n",
       "      <td>0.139816</td>\n",
       "      <td>-1.229676</td>\n",
       "      <td>0.354259</td>\n",
       "      <td>-0.280805</td>\n",
       "      <td>-0.184996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060570</td>\n",
       "      <td>-0.071096</td>\n",
       "      <td>-0.131000</td>\n",
       "      <td>0.269144</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>-0.329489</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.302377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53439</th>\n",
       "      <td>132938</td>\n",
       "      <td>-0.237135</td>\n",
       "      <td>-4.270770</td>\n",
       "      <td>0.337577</td>\n",
       "      <td>-0.351525</td>\n",
       "      <td>-2.731400</td>\n",
       "      <td>1.353471</td>\n",
       "      <td>-0.848786</td>\n",
       "      <td>0.281806</td>\n",
       "      <td>-0.642373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079946</td>\n",
       "      <td>-0.830923</td>\n",
       "      <td>-0.677493</td>\n",
       "      <td>-0.267888</td>\n",
       "      <td>-0.023203</td>\n",
       "      <td>-0.263681</td>\n",
       "      <td>-0.050772</td>\n",
       "      <td>0.153602</td>\n",
       "      <td>11.360302</td>\n",
       "      <td>-0.052985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53812</th>\n",
       "      <td>133885</td>\n",
       "      <td>0.894258</td>\n",
       "      <td>-0.327235</td>\n",
       "      <td>-0.166174</td>\n",
       "      <td>1.135400</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>0.119024</td>\n",
       "      <td>0.240464</td>\n",
       "      <td>0.094359</td>\n",
       "      <td>0.039812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099504</td>\n",
       "      <td>0.059479</td>\n",
       "      <td>-0.289118</td>\n",
       "      <td>-0.303268</td>\n",
       "      <td>0.692327</td>\n",
       "      <td>-0.256227</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>0.020037</td>\n",
       "      <td>1.809684</td>\n",
       "      <td>-0.048532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30180</th>\n",
       "      <td>75065</td>\n",
       "      <td>1.214132</td>\n",
       "      <td>0.271087</td>\n",
       "      <td>-0.292709</td>\n",
       "      <td>0.880706</td>\n",
       "      <td>0.287801</td>\n",
       "      <td>-0.252426</td>\n",
       "      <td>0.246390</td>\n",
       "      <td>0.049230</td>\n",
       "      <td>-0.258470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040439</td>\n",
       "      <td>0.092159</td>\n",
       "      <td>-0.147433</td>\n",
       "      <td>-0.334491</td>\n",
       "      <td>0.735434</td>\n",
       "      <td>-0.257980</td>\n",
       "      <td>-0.006837</td>\n",
       "      <td>-0.009787</td>\n",
       "      <td>-0.294977</td>\n",
       "      <td>-0.338456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45385</th>\n",
       "      <td>112774</td>\n",
       "      <td>-1.368851</td>\n",
       "      <td>1.514854</td>\n",
       "      <td>0.679011</td>\n",
       "      <td>1.154238</td>\n",
       "      <td>-0.388080</td>\n",
       "      <td>0.551933</td>\n",
       "      <td>-0.139067</td>\n",
       "      <td>1.120345</td>\n",
       "      <td>-0.786197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061335</td>\n",
       "      <td>0.160857</td>\n",
       "      <td>-0.161947</td>\n",
       "      <td>-0.299121</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>-0.176876</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.150213</td>\n",
       "      <td>-0.139710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>45744</td>\n",
       "      <td>-0.340397</td>\n",
       "      <td>0.168612</td>\n",
       "      <td>1.933920</td>\n",
       "      <td>-1.040889</td>\n",
       "      <td>-0.666100</td>\n",
       "      <td>-0.125930</td>\n",
       "      <td>-0.160540</td>\n",
       "      <td>0.217909</td>\n",
       "      <td>-1.653811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072602</td>\n",
       "      <td>0.022220</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.176385</td>\n",
       "      <td>-0.401081</td>\n",
       "      <td>-0.397603</td>\n",
       "      <td>0.113489</td>\n",
       "      <td>0.097935</td>\n",
       "      <td>-0.139733</td>\n",
       "      <td>-0.495917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77343</th>\n",
       "      <td>193050</td>\n",
       "      <td>-1.593818</td>\n",
       "      <td>1.234481</td>\n",
       "      <td>2.974995</td>\n",
       "      <td>4.344022</td>\n",
       "      <td>-0.261743</td>\n",
       "      <td>1.101896</td>\n",
       "      <td>-0.189561</td>\n",
       "      <td>0.169217</td>\n",
       "      <td>-0.638361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046710</td>\n",
       "      <td>0.851342</td>\n",
       "      <td>-0.003825</td>\n",
       "      <td>0.064439</td>\n",
       "      <td>0.243371</td>\n",
       "      <td>0.566316</td>\n",
       "      <td>0.679676</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>0.149794</td>\n",
       "      <td>0.531949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97711</th>\n",
       "      <td>244205</td>\n",
       "      <td>1.954993</td>\n",
       "      <td>-0.380506</td>\n",
       "      <td>-0.272158</td>\n",
       "      <td>0.465629</td>\n",
       "      <td>-0.686380</td>\n",
       "      <td>-0.561738</td>\n",
       "      <td>-0.500919</td>\n",
       "      <td>-0.149802</td>\n",
       "      <td>0.959276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280349</td>\n",
       "      <td>1.051016</td>\n",
       "      <td>0.091925</td>\n",
       "      <td>0.093997</td>\n",
       "      <td>-0.191388</td>\n",
       "      <td>0.638577</td>\n",
       "      <td>-0.015144</td>\n",
       "      <td>-0.043221</td>\n",
       "      <td>0.069867</td>\n",
       "      <td>0.793689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91074 rows × 31 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5b09122-4da0-44e6-b6c9-b7ee797b0535')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f5b09122-4da0-44e6-b6c9-b7ee797b0535 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f5b09122-4da0-44e6-b6c9-b7ee797b0535');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            ID        V1        V2        V3        V4        V5        V6  \\\n",
       "109831  274720 -0.395676  0.777782  0.165311 -0.851634  1.117452 -1.398650   \n",
       "205        537  1.131541 -0.174361  0.992621  0.600032 -1.021934 -0.644779   \n",
       "32744    81491  1.238043  0.770530 -0.331998  1.345465  0.139816 -1.229676   \n",
       "53439   132938 -0.237135 -4.270770  0.337577 -0.351525 -2.731400  1.353471   \n",
       "53812   133885  0.894258 -0.327235 -0.166174  1.135400 -0.030057  0.119024   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "30180    75065  1.214132  0.271087 -0.292709  0.880706  0.287801 -0.252426   \n",
       "45385   112774 -1.368851  1.514854  0.679011  1.154238 -0.388080  0.551933   \n",
       "18373    45744 -0.340397  0.168612  1.933920 -1.040889 -0.666100 -0.125930   \n",
       "77343   193050 -1.593818  1.234481  2.974995  4.344022 -0.261743  1.101896   \n",
       "97711   244205  1.954993 -0.380506 -0.272158  0.465629 -0.686380 -0.561738   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "109831  1.710514 -0.852957  0.258354  ...  0.136380  0.842693 -0.305138   \n",
       "205    -0.428619 -0.032071  0.516318  ...  0.022068 -0.031149  0.022845   \n",
       "32744   0.354259 -0.280805 -0.184996  ... -0.060570 -0.071096 -0.131000   \n",
       "53439  -0.848786  0.281806 -0.642373  ...  0.079946 -0.830923 -0.677493   \n",
       "53812   0.240464  0.094359  0.039812  ...  0.099504  0.059479 -0.289118   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "30180   0.246390  0.049230 -0.258470  ...  0.040439  0.092159 -0.147433   \n",
       "45385  -0.139067  1.120345 -0.786197  ...  0.061335  0.160857 -0.161947   \n",
       "18373  -0.160540  0.217909 -1.653811  ...  0.072602  0.022220  0.007301   \n",
       "77343  -0.189561  0.169217 -0.638361  ... -0.046710  0.851342 -0.003825   \n",
       "97711  -0.500919 -0.149802  0.959276  ...  0.280349  1.051016  0.091925   \n",
       "\n",
       "             V24       V25       V26       V27       V28        V29       V30  \n",
       "109831 -0.021773  0.377381 -0.281479 -0.511858 -0.432950   0.154964  0.957272  \n",
       "205     0.414591  0.129935  0.387228 -0.016423  0.034255   0.424789 -0.990249  \n",
       "32744   0.269144  0.712100 -0.329489  0.039308  0.058838  -0.293440 -0.302377  \n",
       "53439  -0.267888 -0.023203 -0.263681 -0.050772  0.153602  11.360302 -0.052985  \n",
       "53812  -0.303268  0.692327 -0.256227 -0.012894  0.020037   1.809684 -0.048532  \n",
       "...          ...       ...       ...       ...       ...        ...       ...  \n",
       "30180  -0.334491  0.735434 -0.257980 -0.006837 -0.009787  -0.294977 -0.338456  \n",
       "45385  -0.299121  0.215686 -0.176876  0.013457  0.014831   0.150213 -0.139710  \n",
       "18373   0.176385 -0.401081 -0.397603  0.113489  0.097935  -0.139733 -0.495917  \n",
       "77343   0.064439  0.243371  0.566316  0.679676  0.135785   0.149794  0.531949  \n",
       "97711   0.093997 -0.191388  0.638577 -0.015144 -0.043221   0.069867  0.793689  \n",
       "\n",
       "[91074 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 라벨링 및 전처리\n",
    "class_0_sample = train_df.sample(frac=0.8, replace=False)\n",
    "class_0_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_sample = class_0_sample.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation 데이터 정의\n",
    "val_x = val_df.drop(columns=['ID','Class'])\n",
    "val_y = val_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sample_scaled = scaler.fit_transform(class_0_sample)\n",
    "val_scaled = scaler.transform(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차항 정의\n",
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2, axis=1))\n",
    "    #return MAE\n",
    "    return np.mean(abs(v1 - v2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reparameterization trick\n",
    "# 데이터 분포를 바꿔주는 함수 \n",
    "\n",
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 정의\n",
    "\n",
    "original_dim = sample_scaled.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 15)           465         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 10)           160         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 10)           160         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 10)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 785\n",
      "Trainable params: 785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, z, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 15)                165       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 30)                480       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 645\n",
      "Trainable params: 645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "# Instantiate the decoder model:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full VAE model\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the KL loss function: \n",
    "# VAE의 비용 함수인 KL 발산\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean))\n",
    "    # compute the KL loss\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)\n",
    "    # return the average loss over all \n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    #total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 30)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 10)                785       \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 30)                645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,430\n",
      "Trainable params: 1,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 91074 samples\n",
      "Epoch 1/30\n",
      "91074/91074 [==============================] - 2s 20us/sample - loss: 86.9564\n",
      "Epoch 2/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 27.7936\n",
      "Epoch 3/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 24.1762\n",
      "Epoch 4/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 22.4062\n",
      "Epoch 5/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 21.0582\n",
      "Epoch 6/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 20.0146\n",
      "Epoch 7/30\n",
      "91074/91074 [==============================] - 1s 8us/sample - loss: 19.4230\n",
      "Epoch 8/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 19.0746\n",
      "Epoch 9/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18.8511\n",
      "Epoch 10/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18.7524\n",
      "Epoch 11/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18.6739\n",
      "Epoch 12/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18.5856\n",
      "Epoch 13/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18.4874\n",
      "Epoch 14/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18.3057\n",
      "Epoch 15/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 18.0678\n",
      "Epoch 16/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 17.7860\n",
      "Epoch 17/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 17.5692\n",
      "Epoch 18/30\n",
      "91074/91074 [==============================] - 1s 8us/sample - loss: 17.3284\n",
      "Epoch 19/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 17.1112\n",
      "Epoch 20/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 16.9633\n",
      "Epoch 21/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 16.8538\n",
      "Epoch 22/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 16.7985\n",
      "Epoch 23/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 16.7426\n",
      "Epoch 24/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 16.7184\n",
      "Epoch 25/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 16.6772\n",
      "Epoch 26/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 16.6556\n",
      "Epoch 27/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 16.6321\n",
      "Epoch 28/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 16.5440\n",
      "Epoch 29/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 16.4664\n",
      "Epoch 30/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 16.3361\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "vae_model.compile(optimizer=\"rmsprop\", loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "# Finally, we train the model:\n",
    "results = vae_model.fit(sample_scaled, sample_scaled,\n",
    "                        shuffle=True,\n",
    "                        epochs=30,\n",
    "                        batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAilklEQVR4nO3de5hddX3v8fdnX2b2zGRmchsiJGoiN1GUUCJiQcoR5Qi0gmKpPeITe3ga28dziqfair081j62B089tVrrBYU2WkQoF0O9VUABPSgSIGK4SAATkzSQIeQ2yUxmZu/v+WOtSXaSSTIzyZqd2evzep797HXfv8Um+zPr91vr91NEYGZm+VNodAHMzKwxHABmZjnlADAzyykHgJlZTjkAzMxyygFgZpZTDgCzg5D0L5I+PsZtV0t68+Eex2yyOADMzHLKAWBmllMOAJvy0qqXP5H0qKQdkq6TNEfSdyRtl3SXpBl1279N0mOStki6R9IpdetOl/Rwut9NQGWfz/pNSSvSfe+X9NoJlvn3JT0t6UVJd0g6Ll0uSZ+StFHSNkk/l3Rquu4iSY+nZVsv6UMT+g9mlnIAWLO4DHgLcBLwW8B3gD8Dekj+P/8jAEknATcCH0jXfRv4d0ktklqAbwBfBWYC/5Yel3Tf04HrgfcBs4AvAndIah1PQSW9CfjfwOXAscAa4Ovp6guAc9Pz6E632ZSuuw54X0R0AqcC3x/P55rtywFgzeIfI+L5iFgP/BB4ICIeiYgB4Hbg9HS73wG+FRF3RsQQ8EmgDfh14CygDPxDRAxFxC3Ag3WfsQT4YkQ8EBHViFgK7Er3G493A9dHxMMRsQv4CPAGSfOBIaATeCWgiHgiIjak+w0Br5LUFRGbI+LhcX6u2V4cANYsnq+b7h9lflo6fRzJX9wAREQNWAvMTdetj717SFxTN/1y4INp9c8WSVuAl6b7jce+Zegj+St/bkR8H/gs8E/ARknXSupKN70MuAhYI+leSW8Y5+ea7cUBYHnznyQ/5EBS507yI74e2ADMTZeNeFnd9FrgbyJiet2rPSJuPMwydJBUKa0HiIjPRMQZwKtIqoL+JF3+YERcAhxDUlV18zg/12wvDgDLm5uBiyWdL6kMfJCkGud+4MfAMPBHksqS3gGcWbfvl4A/kPT6tLG2Q9LFkjrHWYYbgd+TtDBtP/hbkiqr1ZJelx6/DOwABoBa2kbxbkndadXVNqB2GP8dzBwAli8R8QvgCuAfgRdIGox/KyIGI2IQeAfwXuBFkvaC2+r2XQ78PkkVzWbg6XTb8ZbhLuAvgVtJrjqOB96Vru4iCZrNJNVEm4C/S9e9B1gtaRvwByRtCWYTJg8IY2aWT74CMDPLKQeAmVlOOQDMzHIq0wCQdJWklelj9x9Il82UdKekVen7jEMcxszMMpBZI3Daf8nXSW6jGwS+S3LnwhLgxYi4RtLVwIyI+PDBjjV79uyYP39+JuU0M2tWDz300AsR0XOg9aUMP/sUknubdwJIupfkFrtLgPPSbZYC9wAHDYD58+ezfPnyzApqZtaMJK052Posq4BWAm+UNEtSO8kj7C8F5tT1bfIcMGe0nSUtkbRc0vLe3t4Mi2lmlk+ZBUBEPAF8AvgeSfXPCqC6zzYBjFoHFRHXRsSiiFjU03PAKxgzM5ugTBuBI+K6iDgjIs4lebLxKeB5SccCpO8bsyyDmZmNLss2ACQdExEbJb2MpP7/LGABsBi4Jn1flmUZzCyfhoaGWLduHQMDA40uSuYqlQrz5s2jXC6Pa79MAwC4VdIskn7M3x8RWyRdA9ws6UqSvk4uz7gMZpZD69ato7Ozk/nz57N3B6/NJSLYtGkT69atY8GCBePaN9MAiIg3jrJsE3B+lp9rZjYwMND0P/4Akpg1axYTuVnGTwKbWdNq9h//ERM9z6YOgNsfWce//uSgt8GameVWUwfAtx7dwA0P/KrRxTCzHNqyZQuf+9znxr3fRRddxJYtW458gUbR1AHQVSmzrX+o0cUwsxw6UAAMDw8fdL9vf/vbTJ8+PaNS7S3ru4AaqqutzLYBB4CZTb6rr76aZ555hoULF1Iul6lUKsyYMYMnn3ySp556iksvvZS1a9cyMDDAVVddxZIlS4A9Xd/09fVx4YUXcs4553D//fczd+5cli1bRltb2xErY9MHQN+uYWq1oFDIR2OQme3vY//+GI//57YjesxXHdfFR3/r1Qdcf80117By5UpWrFjBPffcw8UXX8zKlSt336p5/fXXM3PmTPr7+3nd617HZZddxqxZs/Y6xqpVq7jxxhv50pe+xOWXX86tt97KFVdcccTOocmrgEpEwPaBg19ymZll7cwzz9zrPv3PfOYznHbaaZx11lmsXbuWVatW7bfPggULWLhwIQBnnHEGq1evPqJlavorAIBtA0N0t4/vCTkzax4H+0t9snR0dOyevueee7jrrrv48Y9/THt7O+edd96oTyy3trbuni4Wi/T39x/RMjX5FUDyo7/VDcFmNsk6OzvZvn37qOu2bt3KjBkzaG9v58knn+QnP/nJJJcu0eRXAMnpuSHYzCbbrFmzOPvsszn11FNpa2tjzpw9Pd+/9a1v5Qtf+AKnnHIKJ598MmeddVZDytjUAdA9UgXU7zYAM5t8X/va10Zd3trayne+851R143U88+ePZuVK1fuXv6hD33oiJcvF1VAfhbAzGx/zR0AdY3AZma2t6YOgM7WEpKvAMzyKhl0sPlN9DybOgAKBdHZWmKbnwMwy51KpcKmTZuaPgRGxgOoVCrj3repG4EhqQbybaBm+TNv3jzWrVs3oX7yp5qREcHGq/kDwB3CmeVSuVwe9whZedPUVUCQPAvgRmAzs/01fQB0t5X9HICZ2SgyDQBJ/0vSY5JWSrpRUkXSAkkPSHpa0k2SWrIsQ1fFXUKbmY0mswCQNBf4I2BRRJwKFIF3AZ8APhURJwCbgSuzKgO4EdjM7ECyrgIqAW2SSkA7sAF4E3BLun4pcGmWBeiqlNk5WGWoWsvyY8zMppzMAiAi1gOfBH5F8sO/FXgI2BIRI5Xy64C5o+0vaYmk5ZKWH85tXCMdwnlMADOzvWVZBTQDuARYABwHdABvHev+EXFtRCyKiEU9PT0TLseeDuFcDWRmVi/LKqA3A7+MiN6IGAJuA84GpqdVQgDzgPUZlsFjApiZHUCWAfAr4CxJ7ZIEnA88DvwAeGe6zWJgWYZlcIdwZmYHkGUbwAMkjb0PAz9PP+ta4MPAH0t6GpgFXJdVGaBuUBg/C2BmtpdMu4KIiI8CH91n8bPAmVl+br1uXwGYmY2q6Z8E9qAwZmaja/oAaG8pUizIjcBmZvto+gCQRFfFHcKZme2r6QMAkjuB3AhsZra3XARAd5s7hDMz21cuAqCr4g7hzMz2lY8AaCv5LiAzs33kIwAqZQ8Mb2a2j1wEQDIqmK8AzMzq5SIAutrK7BquMTBUbXRRzMyOGvkIgEraH5DvBDIz2y0fAbB7TAC3A5iZjchHAFTcIZyZ2b7yEQAeFczMbD+5CIDudEwAPwxmZrZHLgJgTxWQ2wDMzEbkIwBcBWRmtp9cBEClXKSlVHAjsJlZnVwEAKTdQfg2UDOz3TILAEknS1pR99om6QOSZkq6U9Kq9H1GVmWo5w7hzMz2llkARMQvImJhRCwEzgB2ArcDVwN3R8SJwN3pfOaSDuEcAGZmIyarCuh84JmIWANcAixNly8FLp2MAnS5Qzgzs71MVgC8C7gxnZ4TERvS6eeAOaPtIGmJpOWSlvf29h52AZJRwdwGYGY2IvMAkNQCvA34t33XRUQAMdp+EXFtRCyKiEU9PT2HXY6uSskPgpmZ1ZmMK4ALgYcj4vl0/nlJxwKk7xsnoQy7q4CSzDEzs8kIgN9lT/UPwB3A4nR6MbBsEspAV6XMcC3o95gAZmZAxgEgqQN4C3Bb3eJrgLdIWgW8OZ3PXLe7hDYz20spy4NHxA5g1j7LNpHcFTSputr2DArzku7KZH+8mdlRJ1dPAoN7BDUzG5GfAHCHcGZme8lPAHhcYDOzveQmANwIbGa2t9wEQKfbAMzM9pKbAGgpFWgrF90GYGaWyk0AQNoltNsAzMyAnAVAd5sHhTEzG5GrAPCYAGZme+QrANrKbgQ2M0vlKwAqbgMwMxuRqwBwG4CZ2R65CoCutjLbB4ao1TwmgJlZvgKgUqYW0DfoqwAzs3wFwEiX0G4INjPLWQBU3B+QmdmIXAXA7g7hfCeQmVm+AmBkTAA/C2BmlrcAqHhQGDOzEVkPCj9d0i2SnpT0hKQ3SJop6U5Jq9L3GVmWod6ecYHdBmBmlvUVwKeB70bEK4HTgCeAq4G7I+JE4O50flJ0+grAzGy3zAJAUjdwLnAdQEQMRsQW4BJgabrZUuDSrMqwr2JBdLa6OwgzM8j2CmAB0Av8s6RHJH1ZUgcwJyI2pNs8B8wZbWdJSyQtl7S8t7f3iBXKHcKZmSWyDIAS8GvA5yPidGAH+1T3REQAo/bLEBHXRsSiiFjU09NzxArVWSn5OQAzM7INgHXAuoh4IJ2/hSQQnpd0LED6vjHDMuynq81jApiZQYYBEBHPAWslnZwuOh94HLgDWJwuWwwsy6oMo0l6BHUAmJmVMj7+/wRukNQCPAv8Hkno3CzpSmANcHnGZdhLV8UBYGYGGQdARKwAFo2y6vwsP/dgkoHh3QZgZparJ4EhuQLo2zXMcLXW6KKYmTVU7gJgpEO4vl2+CjCzfMtdAIx0COdbQc0s7/IXAJWk2cMPg5lZ3uUvADwmgJkZkMcAcIdwZmZADgOgu91XAGZmkMMAcBuAmVkidwHQ0VKiIN8FZGaWuwAoFERnxR3CmZnlLgDAHcKZmcEYA0DSVZK6lLhO0sOSLsi6cFlxf0BmZmO/AvjvEbENuACYAbwHuCazUmWsq+JRwczMxhoASt8vAr4aEY/VLZty3CW0mdnYA+AhSd8jCYD/kNQJTNnuNJMqIAeAmeXbWMcDuBJYCDwbETslzSQZ3GVKShqB3QZgZvk21iuANwC/iIgtkq4A/gLYml2xstVVKdM/VGVweMpexJiZHbaxBsDngZ2STgM+CDwDfCWzUmXMHcKZmY09AIYjIoBLgM9GxD8BndkVK1tdbUnNlxuCzSzPxtoGsF3SR0hu/3yjpAJQPtROklYD24EqSYgsStsPbgLmA6uByyNi8/iLPnHdu68A3A5gZvk11iuA3wF2kTwP8BwwD/i7Me77XyJiYUSMDA5/NXB3RJwI3J3OTyp3CW1mNsYASH/0bwC6Jf0mMBARE20DuARYmk4vBS6d4HEmbKQNwA+DmVmejbUriMuBnwK/DVwOPCDpnWPYNYDvSXpI0pJ02ZyI2JBOPwfMOcBnLpG0XNLy3t7esRRzzHZfAbgR2MxybKxtAH8OvC4iNgJI6gHuAm45xH7nRMR6SccAd0p6sn5lRISkGG3HiLgWuBZg0aJFo24zUd0eGN7MbMxtAIWRH//UprHsGxHr0/eNwO3AmcDzko4FSN83HvgI2aiUC5SL8hWAmeXaWAPgu5L+Q9J7Jb0X+Bbw7YPtIKkj7TICSR0kHcmtBO4AFqebLQaWTaTgh0OSO4Qzs9wbUxVQRPyJpMuAs9NF10bE7YfYbQ5wu6SRz/laRHxX0oPAzZKuBNaQtClMui6PCWBmOTfWNgAi4lbg1nFs/yxw2ijLNwHnj/U4WemqeEwAM8u3gwaApO0kd/Lst4qkDbcrk1JNAl8BmFneHTQAImLKdvdwKF1tZdZv6W90MczMGiaXYwKDB4UxM8tvALSV2NY/TNLHnZlZ/uQ2ALrbygxWa+zymABmllO5DQB3CGdmeZffAHCHcGaWc/kNgEo6KIy7gzCznMpvALhDODPLudwGQLfHBTaznMttALgR2MzyLrcB0Jm2AbgR2MzyKrcBUCkXaS0V3CGcmeVWbgMAknYAVwGZWV7lOgC62spuBDaz3Mp3AFRKbgMws9zKdwC0lf0cgJnlVr4DoOIqIDPLr1wHgBuBzSzPMg8ASUVJj0j6Zjq/QNIDkp6WdJOklqzLcCBdbcm4wB4TwMzyaDKuAK4Cnqib/wTwqYg4AdgMXDkJZRhVV6VMtRbsGKw2qghmZg2TaQBImgdcDHw5nRfwJuCWdJOlwKVZluFg9nQI52ogM8ufrK8A/gH4U2Bk2K1ZwJaIGLn1Zh0wd7QdJS2RtFzS8t7e3kwK5w7hzCzPMgsASb8JbIyIhyayf0RcGxGLImJRT0/PES5dYk+HcL4V1Mzyp5Thsc8G3ibpIqACdAGfBqZLKqVXAfOA9RmW4aC62twhnJnlV2ZXABHxkYiYFxHzgXcB34+IdwM/AN6ZbrYYWJZVGQ7FXUKbWZ414jmADwN/LOlpkjaB6xpQBqCuEdhtAGaWQ1lWAe0WEfcA96TTzwJnTsbnHsrucYHdBmBmOZTrJ4FLxQIdLUVfAZhZLuU6ACCpBnIjsJnlkQOg4v6AzCyfch8A3R4UxsxyKvcB0NVWciOwmeWSA6DiNgAzyycHgKuAzCynHACVEn27hqnVPCaAmeWLA6CtTARs3+V2ADPLFweAxwQws5xyAKQdwrkh2MzyxgGQdgnthmAzy5vcB8DuUcH8LICZ5UzuA2D3mAC+AjCznHEAuBHYzHIq9wHQ2VpCcgCYWf7kPgAKBdHZWmLbgNsAzCxfch8AkHYH4SsAM8sZBwDpmABuBDaznMksACRVJP1U0s8kPSbpY+nyBZIekPS0pJsktWRVhrHqaiv5QTAzy50srwB2AW+KiNOAhcBbJZ0FfAL4VEScAGwGrsywDGOSjArmNgAzy5fMAiASfelsOX0F8CbglnT5UuDSrMowVh4VzMzyKNM2AElFSSuAjcCdwDPAlogY+XN7HTD3APsukbRc0vLe3t4si+lGYDPLpUwDICKqEbEQmAecCbxyHPteGxGLImJRT09PVkUEkiqgHYNVhqq1TD/HzOxoMil3AUXEFuAHwBuA6ZJK6ap5wPrJKMPBjHQIt93PAphZjmR5F1CPpOnpdBvwFuAJkiB4Z7rZYmBZVmUYq253B2FmOVQ69CYTdiywVFKRJGhujohvSnoc+LqkjwOPANdlWIYxcYdwZpZHmQVARDwKnD7K8mdJ2gOOGiMdwj2xYRuvnTe9sYUxM5skfhIYeO28bl4zt5u/+MZKvrtyQ6OLY2Y2KRwAQKVc5Ibffz2vmdvN+7/2CMtWNLxd2swscw6AVFelzFevfD2vmz+DD9y0gpsfXNvoIpmZZcoBUKejtcQ/v/dMzjlhNn9666N89cerG10kM7PMOAD20dZS5MuLF/HmU47hL5c9xpd/+Gyji2RmlgkHwChaS0U+9+4zuPg1x/Lxbz3BZ7+/qtFFMjM74rJ8DmBKaykV+PS7FtJaKvDJ7z3FwFCND15wEpIaXTQzsyPCAXAQpWKBT/72abSUCnz2B08zMFTlzy8+xSFgZk3BAXAIhYL427e/htZSgS//6JfsGq7xsbe9mkLBIWBmU5sDYAwKBfFXb3s1lXKRL973LM9tG+BDF5zMyS/pbHTRzMwmzAEwRpK4+sJXMrOjhU/fvYo7H3+eN59yDH943gmc8fIZjS6emdm4KSIaXYZDWrRoUSxfvrzRxdhty85Blt6/hn+5/5ds3jnE6xfM5A/PO57fOKnH7QNmdtSQ9FBELDrgegfAxO0cHObGn67lyz98lg1bB3j1cV384XnHc+Gpx1J0G4GZNZgDYBIMDtf4xiPr+cK9z/DsCzuYP6ud9/3G8bzj1+bSWio2unhmllMOgElUrQX/8dhzfO6ep1m5fhvHdLZy2RnzeMfpczlxjhuMzWxyOQAaICL44aoXuP7//ZIfrnqBai149XFdvP30ubzttOM4pqvS6CKaWQ44ABqsd/suvvnof/KNR9bzs3VbKQjOPmE2ly6cy3899SVMa/WNWGaWDQfAUeSZ3j6WPbKe21esZ+2L/VTKBS541Ut4++lzOefE2ZSL7prJzI4cB8BRKCJ4+Febue3h9Xzz0Q1s7R+is1LinBNmc+5JPZx7Ug9zp7c1uphmNsU1LAAkvRT4CjAHCODaiPi0pJnATcB8YDVweURsPtixmi0A6g0O17j3qV7uevx57lvVy4atAwCccMw0zj2xh3NPms1Zr5hFpey7icxsfBoZAMcCx0bEw5I6gYeAS4H3Ai9GxDWSrgZmRMSHD3asZg6AehHBqo193PdUL/c+1csDv3yRweEaraUCZy6YyW+c1MMbT+zh+J4OSq4uMrNDOGqqgCQtAz6bvs6LiA1pSNwTEScfbN+8BMC++gerPPDLTdz31Avct6qXpzf2AdBSLDB/djvH90zjhGOmcXxP8npFTwcdblQ2s9RREQCS5gP3AacCv4qI6elyAZtH5vfZZwmwBOBlL3vZGWvWrMm8nEe79Vv6+fEzm3h6Yx/P9PbxzMY+1ry4k2ptz3d4XHeF49NQeNnMdno6W5k9rZWezuTVVSm5uwqznGh4AEiaBtwL/E1E3CZpS/0PvqTNEXHQ3tTyegUwFoPDNdZs2pEEQu+OvcJhx2B1v+1bSgV6prUyu7OVnpFgmNZCV1uZrkqZzkqJzkqZrrbkPZkv+YlmsynoUAGQaX2BpDJwK3BDRNyWLn5e0rF1VUAbsyxDs2spFThxTud+TxpHBFv7h+jdvit59e3ab3rd5p2sWLuZTTsGOdTfAa2lAp2VMu0tRVpKBVqKBcqlAq3FAi2lAuWikuWlIuWiaC0VKBUKlIvJulJR6XSBUkF1y5NlyTH3bJMs22d+v89KlvmKxmxiMguAtHrnOuCJiPj7ulV3AIuBa9L3ZVmVIc8kMb29hentLYfshqJaC/p2DbN9YIjtA8NsHxhmW/8Q23fVzQ8Msa1/mIGhKoPDNXYN1xis1hgartE/VGVrf43B4RpD1T3rhqs1hqrBULXGcC32qqo6kspF7Q6kkZColIt0tJboaNnnvbVER0uJjtZkvr2luPvqp7stufLpqpR915XlQpZXAGcD7wF+LmlFuuzPSH74b5Z0JbAGuDzDMtgYFAuiuy35AcxSrRYM1WoMp6GwOxyqkYRJ3WtwOOqmk0BJAib2W7ZnXTo/XGNguErfrio7dw3z4o6d7ByssnNwmL5dwwwM1Q5Z1pZSIQ2F0l4BMaO9zIyOFma0tzC9vcyM9hZmduyZbm8p+orEpozMAiAifgQc6F/C+Vl9rh29CgXRWijS6BuVqrVgx+AwO3dV6ds1xLb0imfkfWv/0O4rnuR9iC07B1m9aQebdwyybWD4gMduKRaY3l6mq63MtNak/WSkbSWZ39Ou0lkp01VJAia5+ijT2VrycKM2aXzPoOVOsSC6Kslf9TD+jvmGqzW2pKGweecQm3cMsnlkeucgW3bsqT7bNjDM+i399KVVaf1D+zfM15Ogs7UuFOqqpjrrgqS+wX7a7kBJlreW3C5iY+MAMBunUrHA7GnJ7bXjNVSt7Q6DbWmby54rjqFRr0SefaGPbf1JG81od3btq1gQbeUilXKRtpYCbeVi3Xwy3daSzLemjemtpWR65DWyrCWdL6WN98W6V2m/6WSbQt26/fcpUBAOqKOEA8BsEpWLhaQNoaNlQvtXa0FfGh5Jw319430SHv2DVfqHquwcrDIwVN093z9U5cUdg3vmB9MG/bTtZDKVCqIgIXHQ95GwKAiKSsKloCRMCiJ933uZ6o9D8o5I1iEKheR9tM+CkWOw13FHvystnS/tudut/i63PXe+JcFXSm9WKKV3tFXKyc0KySsJ3MkeSdABYDaFFAuiu71Md/uRbbCPSBrid6WN6Hveq+waqjFcq1Gtkb4Hw7Wglr5X615D1Rq1CKo1qNb23P1Vv91wLRiu1qgFBEFEcoNAALVI5+veaxHUalDdPR1U032Sz6p/Tzoei1GPAxE1opouB2rJxrvLUqsl62DPPrVa/U0KsdeNB0dauSgqpSKtaShUykWuW7yIl8/qOOKfBQ4AMyP5qzmpBvLtr2MVMRJ6sTsQhuvuchuuRbos9rklOr1TbajGwFBylTYwPDKdvI8E78BwlbYMb0l2AJiZTYCUPOBYKkIbUzM43aWkmVlOOQDMzHLKAWBmllMOADOznHIAmJnllAPAzCynHABmZjnlADAzy6lJGxT+cEjqJRk7YCJmAy8cweIcDZrtnHw+R79mO6dmOx8Y/ZxeHhE9B9phSgTA4ZC0/GBjYk5FzXZOPp+jX7OdU7OdD0zsnFwFZGaWUw4AM7OcykMAXNvoAmSg2c7J53P0a7ZzarbzgQmcU9O3AZiZ2ejycAVgZmajcACYmeVUUweApLdK+oWkpyVd3ejyHC5JqyX9XNIKScsbXZ6JkHS9pI2SVtYtmynpTkmr0vcZjSzjeBzgfP5K0vr0e1oh6aJGlnE8JL1U0g8kPS7pMUlXpcun8nd0oHOakt+TpIqkn0r6WXo+H0uXL5D0QPp7d5OkQw483bRtAJKKwFPAW4B1wIPA70bE4w0t2GGQtBpYFBFT9gEWSecCfcBXIuLUdNn/AV6MiGvSoJ4RER9uZDnH6gDn81dAX0R8spFlmwhJxwLHRsTDkjqBh4BLgfcydb+jA53T5UzB70mSgI6I6JNUBn4EXAX8MXBbRHxd0heAn0XE5w92rGa+AjgTeDoino2IQeDrwCUNLlPuRcR9wIv7LL4EWJpOLyX5xzklHOB8pqyI2BARD6fT24EngLlM7e/oQOc0JUWiL50tp68A3gTcki4f03fUzAEwF1hbN7+OKfylpwL4nqSHJC1pdGGOoDkRsSGdfg6Y08jCHCH/Q9KjaRXRlKkuqSdpPnA68ABN8h3tc04wRb8nSUVJK4CNwJ3AM8CWiBhONxnT710zB0AzOicifg24EHh/Wv3QVCKpk5zq9ZKfB44HFgIbgP/b0NJMgKRpwK3AByJiW/26qfodjXJOU/Z7iohqRCwE5pHUdrxyIsdp5gBYD7y0bn5eumzKioj16ftG4HaSL74ZPJ/W047U125scHkOS0Q8n/4DrQFfYop9T2m98q3ADRFxW7p4Sn9Ho53TVP+eACJiC/AD4A3AdEmldNWYfu+aOQAeBE5MW8ZbgHcBdzS4TBMmqSNtwEJSB3ABsPLge00ZdwCL0+nFwLIGluWwjfxQpt7OFPqe0gbG64AnIuLv61ZN2e/oQOc0Vb8nST2SpqfTbSQ3ujxBEgTvTDcb03fUtHcBAaS3df0DUASuj4i/aWyJJk7SK0j+6gcoAV+biucj6UbgPJKua58HPgp8A7gZeBlJt9+XR8SUaFg9wPmcR1KtEMBq4H119edHNUnnAD8Efg7U0sV/RlJnPlW/owOd0+8yBb8nSa8laeQtkvwRf3NE/HX6G/F1YCbwCHBFROw66LGaOQDMzOzAmrkKyMzMDsIBYGaWUw4AM7OccgCYmeWUA8DMLKccAGYZk3SepG82uhxm+3IAmJnllAPALCXpirSf9RWSvph2uNUn6VNpv+t3S+pJt10o6SdpR2K3j3QkJukESXelfbU/LOn49PDTJN0i6UlJN6RPp5o1lAPADJB0CvA7wNlpJ1tV4N1AB7A8Il4N3EvypC/AV4APR8RrSZ4wHVl+A/BPEXEa8OsknYxB0gPlB4BXAa8Azs74lMwOqXToTcxy4XzgDODB9I/zNpIOz2rATek2/wrcJqkbmB4R96bLlwL/lvbVNDcibgeIiAGA9Hg/jYh16fwKYD7JQB5mDeMAMEsIWBoRH9lrofSX+2w30b5T6vtkqeJ/e3YUcBWQWeJu4J2SjoHdY+C+nOTfyEgPi/8N+FFEbAU2S3pjuvw9wL3paFPrJF2aHqNVUvtknoTZePivEDMgIh6X9BckI64VgCHg/cAO4Mx03UaSdgJIutv9QvoD/yzwe+ny9wBflPTX6TF+exJPw2xc3Buo2UFI6ouIaY0uh1kWXAVkZpZTvgIwM8spXwGYmeWUA8DMLKccAGZmOeUAMDPLKQeAmVlO/X89QZq52ZPLigAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss 그래프로 모델 학습 결과 확인\n",
    "\n",
    "plt.plot(results.history['loss'])\n",
    "#plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "val_x_pred = vae_model.predict(val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg error 0.025625453683638097\n",
      "median error 0.024085804165763454\n",
      "99Q: 0.055438123893974206\n",
      "setting threshold on 0.055438123893974206 \n"
     ]
    }
   ],
   "source": [
    "mae_vector = get_error_term(val_x_pred, val_scaled, _rmse=False)\n",
    "print(f'Avg error {np.mean(mae_vector)}\\nmedian error {np.median(mae_vector)}\\n99Q: {np.quantile(mae_vector, 0.99)}')\n",
    "print(f'setting threshold on { np.quantile(mae_vector, 0.99)} ')\n",
    "\n",
    "error_thresh = np.quantile(mae_vector, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010013351134846462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prediction한 것으로 이상치 찾아내기 \n",
    "anomalies = (mae_vector > error_thresh)\n",
    "np.count_nonzero(anomalies) / len(anomalies) #이상치 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score : [0.5706399257283392]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     28432\n",
      "           1       0.08      0.77      0.15        30\n",
      "\n",
      "    accuracy                           0.99     28462\n",
      "   macro avg       0.54      0.88      0.57     28462\n",
      "weighted avg       1.00      0.99      0.99     28462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 정확도 행렬 확인 \n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "val_score = f1_score(val_y, anomalies, average='macro')\n",
    "\n",
    "print(f'Validation F1 Score : [{val_score}]')\n",
    "print(classification_report(val_y, anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE feature 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 특성 다 사용했을 때의 스코어 = [0.5706399257283392]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3c8d8d38-89e7-4583-b788-54ad855b813f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109831</th>\n",
       "      <td>-0.395676</td>\n",
       "      <td>0.777782</td>\n",
       "      <td>0.165311</td>\n",
       "      <td>-0.851634</td>\n",
       "      <td>1.117452</td>\n",
       "      <td>-1.398650</td>\n",
       "      <td>1.710514</td>\n",
       "      <td>-0.852957</td>\n",
       "      <td>0.258354</td>\n",
       "      <td>-0.766738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136380</td>\n",
       "      <td>0.842693</td>\n",
       "      <td>-0.305138</td>\n",
       "      <td>-0.021773</td>\n",
       "      <td>0.377381</td>\n",
       "      <td>-0.281479</td>\n",
       "      <td>-0.511858</td>\n",
       "      <td>-0.432950</td>\n",
       "      <td>0.154964</td>\n",
       "      <td>0.957272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1.131541</td>\n",
       "      <td>-0.174361</td>\n",
       "      <td>0.992621</td>\n",
       "      <td>0.600032</td>\n",
       "      <td>-1.021934</td>\n",
       "      <td>-0.644779</td>\n",
       "      <td>-0.428619</td>\n",
       "      <td>-0.032071</td>\n",
       "      <td>0.516318</td>\n",
       "      <td>-0.118985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>-0.031149</td>\n",
       "      <td>0.022845</td>\n",
       "      <td>0.414591</td>\n",
       "      <td>0.129935</td>\n",
       "      <td>0.387228</td>\n",
       "      <td>-0.016423</td>\n",
       "      <td>0.034255</td>\n",
       "      <td>0.424789</td>\n",
       "      <td>-0.990249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32744</th>\n",
       "      <td>1.238043</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>-0.331998</td>\n",
       "      <td>1.345465</td>\n",
       "      <td>0.139816</td>\n",
       "      <td>-1.229676</td>\n",
       "      <td>0.354259</td>\n",
       "      <td>-0.280805</td>\n",
       "      <td>-0.184996</td>\n",
       "      <td>-0.699813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060570</td>\n",
       "      <td>-0.071096</td>\n",
       "      <td>-0.131000</td>\n",
       "      <td>0.269144</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>-0.329489</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.302377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53439</th>\n",
       "      <td>-0.237135</td>\n",
       "      <td>-4.270770</td>\n",
       "      <td>0.337577</td>\n",
       "      <td>-0.351525</td>\n",
       "      <td>-2.731400</td>\n",
       "      <td>1.353471</td>\n",
       "      <td>-0.848786</td>\n",
       "      <td>0.281806</td>\n",
       "      <td>-0.642373</td>\n",
       "      <td>0.783301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079946</td>\n",
       "      <td>-0.830923</td>\n",
       "      <td>-0.677493</td>\n",
       "      <td>-0.267888</td>\n",
       "      <td>-0.023203</td>\n",
       "      <td>-0.263681</td>\n",
       "      <td>-0.050772</td>\n",
       "      <td>0.153602</td>\n",
       "      <td>11.360302</td>\n",
       "      <td>-0.052985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53812</th>\n",
       "      <td>0.894258</td>\n",
       "      <td>-0.327235</td>\n",
       "      <td>-0.166174</td>\n",
       "      <td>1.135400</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>0.119024</td>\n",
       "      <td>0.240464</td>\n",
       "      <td>0.094359</td>\n",
       "      <td>0.039812</td>\n",
       "      <td>0.030640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099504</td>\n",
       "      <td>0.059479</td>\n",
       "      <td>-0.289118</td>\n",
       "      <td>-0.303268</td>\n",
       "      <td>0.692327</td>\n",
       "      <td>-0.256227</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>0.020037</td>\n",
       "      <td>1.809684</td>\n",
       "      <td>-0.048532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30180</th>\n",
       "      <td>1.214132</td>\n",
       "      <td>0.271087</td>\n",
       "      <td>-0.292709</td>\n",
       "      <td>0.880706</td>\n",
       "      <td>0.287801</td>\n",
       "      <td>-0.252426</td>\n",
       "      <td>0.246390</td>\n",
       "      <td>0.049230</td>\n",
       "      <td>-0.258470</td>\n",
       "      <td>0.221705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040439</td>\n",
       "      <td>0.092159</td>\n",
       "      <td>-0.147433</td>\n",
       "      <td>-0.334491</td>\n",
       "      <td>0.735434</td>\n",
       "      <td>-0.257980</td>\n",
       "      <td>-0.006837</td>\n",
       "      <td>-0.009787</td>\n",
       "      <td>-0.294977</td>\n",
       "      <td>-0.338456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45385</th>\n",
       "      <td>-1.368851</td>\n",
       "      <td>1.514854</td>\n",
       "      <td>0.679011</td>\n",
       "      <td>1.154238</td>\n",
       "      <td>-0.388080</td>\n",
       "      <td>0.551933</td>\n",
       "      <td>-0.139067</td>\n",
       "      <td>1.120345</td>\n",
       "      <td>-0.786197</td>\n",
       "      <td>-0.327337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061335</td>\n",
       "      <td>0.160857</td>\n",
       "      <td>-0.161947</td>\n",
       "      <td>-0.299121</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>-0.176876</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.150213</td>\n",
       "      <td>-0.139710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>-0.340397</td>\n",
       "      <td>0.168612</td>\n",
       "      <td>1.933920</td>\n",
       "      <td>-1.040889</td>\n",
       "      <td>-0.666100</td>\n",
       "      <td>-0.125930</td>\n",
       "      <td>-0.160540</td>\n",
       "      <td>0.217909</td>\n",
       "      <td>-1.653811</td>\n",
       "      <td>0.429305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072602</td>\n",
       "      <td>0.022220</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.176385</td>\n",
       "      <td>-0.401081</td>\n",
       "      <td>-0.397603</td>\n",
       "      <td>0.113489</td>\n",
       "      <td>0.097935</td>\n",
       "      <td>-0.139733</td>\n",
       "      <td>-0.495917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77343</th>\n",
       "      <td>-1.593818</td>\n",
       "      <td>1.234481</td>\n",
       "      <td>2.974995</td>\n",
       "      <td>4.344022</td>\n",
       "      <td>-0.261743</td>\n",
       "      <td>1.101896</td>\n",
       "      <td>-0.189561</td>\n",
       "      <td>0.169217</td>\n",
       "      <td>-0.638361</td>\n",
       "      <td>2.511233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046710</td>\n",
       "      <td>0.851342</td>\n",
       "      <td>-0.003825</td>\n",
       "      <td>0.064439</td>\n",
       "      <td>0.243371</td>\n",
       "      <td>0.566316</td>\n",
       "      <td>0.679676</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>0.149794</td>\n",
       "      <td>0.531949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97711</th>\n",
       "      <td>1.954993</td>\n",
       "      <td>-0.380506</td>\n",
       "      <td>-0.272158</td>\n",
       "      <td>0.465629</td>\n",
       "      <td>-0.686380</td>\n",
       "      <td>-0.561738</td>\n",
       "      <td>-0.500919</td>\n",
       "      <td>-0.149802</td>\n",
       "      <td>0.959276</td>\n",
       "      <td>-0.012544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280349</td>\n",
       "      <td>1.051016</td>\n",
       "      <td>0.091925</td>\n",
       "      <td>0.093997</td>\n",
       "      <td>-0.191388</td>\n",
       "      <td>0.638577</td>\n",
       "      <td>-0.015144</td>\n",
       "      <td>-0.043221</td>\n",
       "      <td>0.069867</td>\n",
       "      <td>0.793689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91074 rows × 30 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c8d8d38-89e7-4583-b788-54ad855b813f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3c8d8d38-89e7-4583-b788-54ad855b813f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3c8d8d38-89e7-4583-b788-54ad855b813f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "109831 -0.395676  0.777782  0.165311 -0.851634  1.117452 -1.398650  1.710514   \n",
       "205     1.131541 -0.174361  0.992621  0.600032 -1.021934 -0.644779 -0.428619   \n",
       "32744   1.238043  0.770530 -0.331998  1.345465  0.139816 -1.229676  0.354259   \n",
       "53439  -0.237135 -4.270770  0.337577 -0.351525 -2.731400  1.353471 -0.848786   \n",
       "53812   0.894258 -0.327235 -0.166174  1.135400 -0.030057  0.119024  0.240464   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "30180   1.214132  0.271087 -0.292709  0.880706  0.287801 -0.252426  0.246390   \n",
       "45385  -1.368851  1.514854  0.679011  1.154238 -0.388080  0.551933 -0.139067   \n",
       "18373  -0.340397  0.168612  1.933920 -1.040889 -0.666100 -0.125930 -0.160540   \n",
       "77343  -1.593818  1.234481  2.974995  4.344022 -0.261743  1.101896 -0.189561   \n",
       "97711   1.954993 -0.380506 -0.272158  0.465629 -0.686380 -0.561738 -0.500919   \n",
       "\n",
       "              V8        V9       V10  ...       V21       V22       V23  \\\n",
       "109831 -0.852957  0.258354 -0.766738  ...  0.136380  0.842693 -0.305138   \n",
       "205    -0.032071  0.516318 -0.118985  ...  0.022068 -0.031149  0.022845   \n",
       "32744  -0.280805 -0.184996 -0.699813  ... -0.060570 -0.071096 -0.131000   \n",
       "53439   0.281806 -0.642373  0.783301  ...  0.079946 -0.830923 -0.677493   \n",
       "53812   0.094359  0.039812  0.030640  ...  0.099504  0.059479 -0.289118   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "30180   0.049230 -0.258470  0.221705  ...  0.040439  0.092159 -0.147433   \n",
       "45385   1.120345 -0.786197 -0.327337  ...  0.061335  0.160857 -0.161947   \n",
       "18373   0.217909 -1.653811  0.429305  ...  0.072602  0.022220  0.007301   \n",
       "77343   0.169217 -0.638361  2.511233  ... -0.046710  0.851342 -0.003825   \n",
       "97711  -0.149802  0.959276 -0.012544  ...  0.280349  1.051016  0.091925   \n",
       "\n",
       "             V24       V25       V26       V27       V28        V29       V30  \n",
       "109831 -0.021773  0.377381 -0.281479 -0.511858 -0.432950   0.154964  0.957272  \n",
       "205     0.414591  0.129935  0.387228 -0.016423  0.034255   0.424789 -0.990249  \n",
       "32744   0.269144  0.712100 -0.329489  0.039308  0.058838  -0.293440 -0.302377  \n",
       "53439  -0.267888 -0.023203 -0.263681 -0.050772  0.153602  11.360302 -0.052985  \n",
       "53812  -0.303268  0.692327 -0.256227 -0.012894  0.020037   1.809684 -0.048532  \n",
       "...          ...       ...       ...       ...       ...        ...       ...  \n",
       "30180  -0.334491  0.735434 -0.257980 -0.006837 -0.009787  -0.294977 -0.338456  \n",
       "45385  -0.299121  0.215686 -0.176876  0.013457  0.014831   0.150213 -0.139710  \n",
       "18373   0.176385 -0.401081 -0.397603  0.113489  0.097935  -0.139733 -0.495917  \n",
       "77343   0.064439  0.243371  0.566316  0.679676  0.135785   0.149794  0.531949  \n",
       "97711   0.093997 -0.191388  0.638577 -0.015144 -0.043221   0.069867  0.793689  \n",
       "\n",
       "[91074 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_0_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 진행\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=25)\n",
    "\n",
    "sample_pca = pca.fit_transform(class_0_sample)\n",
    "sample_pca = pd.DataFrame(sample_pca)\n",
    "\n",
    "val_x_pca = pca.transform(val_x)\n",
    "val_x_pca = pd.DataFrame(val_x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sample_pca_scaled = scaler.fit_transform(sample_pca)\n",
    "val_pca_scaled = scaler.transform(val_x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 정의\n",
    "\n",
    "original_dim = sample_pca_scaled.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, z, name='encoder')\n",
    "\n",
    "# decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "# Instantiate the decoder model:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# full VAE model\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "vae_model.compile(optimizer=\"rmsprop\", loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "# Finally, we train the model:\n",
    "results = vae_model.fit(sample_pca_scaled, sample_pca_scaled,\n",
    "                        shuffle=True,\n",
    "                        epochs=30,\n",
    "                        batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss 그래프로 모델 학습 결과 확인\n",
    "\n",
    "plt.plot(results.history['loss'])\n",
    "#plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg error 0.02447385257123011\n",
      "median error 0.023146402778433682\n",
      "99Q: 0.05415718543572091\n",
      "setting threshold on 0.05415718543572091 \n"
     ]
    }
   ],
   "source": [
    "# 예측 및 문턱값 정의\n",
    "\n",
    "val_pca_pred = vae_model.predict(val_pca_scaled)\n",
    "\n",
    "mae_vector = get_error_term(val_pca_pred, val_pca_scaled, _rmse=False)\n",
    "\n",
    "print(f'Avg error {np.mean(mae_vector)}\\nmedian error {np.median(mae_vector)}\\n99Q: {np.quantile(mae_vector, 0.99)}')\n",
    "print(f'setting threshold on { np.quantile(mae_vector, 0.99)} ')\n",
    "\n",
    "error_thresh = np.quantile(mae_vector, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010013351134846462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이상치 정의 및 비율 파악\n",
    "anomalies = (mae_vector > error_thresh)\n",
    "np.count_nonzero(anomalies) / len(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score : [0.5706399257283392]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     28432\n",
      "           1       0.08      0.77      0.15        30\n",
      "\n",
      "    accuracy                           0.99     28462\n",
      "   macro avg       0.54      0.88      0.57     28462\n",
      "weighted avg       1.00      0.99      0.99     28462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "# from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "val_score_pca = f1_score(val_y, anomalies, average='macro')\n",
    "\n",
    "print(f'Validation F1 Score : [{val_score_pca}]')\n",
    "print(classification_report(val_y, anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score : [0.5706399257283392]\n"
     ]
    }
   ],
   "source": [
    "# 모든 feature 사용했을 때와 완전히 동일\n",
    "print(f'Validation F1 Score : [{val_score}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "selected_feature = [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 16, 17, 18, 21, 27, 30]\n",
    "feature_num = [selected_feature[i] - 1 for i in range(len(selected_feature))]\n",
    "\n",
    "train_df_2 = class_0_sample.iloc[:, feature_num]\n",
    "val_df_2 = val_x.iloc[:, feature_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_2_scaled = scaler.fit_transform(train_df_2)\n",
    "val_2_scaled = scaler.transform(val_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 정의\n",
    "\n",
    "original_dim = train_2_scaled.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, z, name='encoder')\n",
    "\n",
    "# decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "# Instantiate the decoder model:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# full VAE model\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 18)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 6)                 291       \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 18)                243       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 534\n",
      "Trainable params: 534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 91074 samples\n",
      "Epoch 1/30\n",
      "91074/91074 [==============================] - 1s 11us/sample - loss: 68.0453\n",
      "Epoch 2/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 18.3175\n",
      "Epoch 3/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 11.8974\n",
      "Epoch 4/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 9.7913\n",
      "Epoch 5/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.9746\n",
      "Epoch 6/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.7785\n",
      "Epoch 7/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.7066\n",
      "Epoch 8/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.6592\n",
      "Epoch 9/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.6482\n",
      "Epoch 10/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 8.6308\n",
      "Epoch 11/30\n",
      "91074/91074 [==============================] - 1s 8us/sample - loss: 8.6246\n",
      "Epoch 12/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 8.6095\n",
      "Epoch 13/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 8.5867\n",
      "Epoch 14/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 8.5759\n",
      "Epoch 15/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8.5505\n",
      "Epoch 16/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.5291\n",
      "Epoch 17/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8.4965\n",
      "Epoch 18/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8.4561\n",
      "Epoch 19/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.4209\n",
      "Epoch 20/30\n",
      "91074/91074 [==============================] - 1s 6us/sample - loss: 8.3807\n",
      "Epoch 21/30\n",
      "91074/91074 [==============================] - 1s 11us/sample - loss: 8.3718\n",
      "Epoch 22/30\n",
      "91074/91074 [==============================] - 1s 10us/sample - loss: 8.3510\n",
      "Epoch 23/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8.3440\n",
      "Epoch 24/30\n",
      "91074/91074 [==============================] - 1s 12us/sample - loss: 8.3322\n",
      "Epoch 25/30\n",
      "91074/91074 [==============================] - 1s 11us/sample - loss: 8.3222\n",
      "Epoch 26/30\n",
      "91074/91074 [==============================] - 1s 11us/sample - loss: 8.3188\n",
      "Epoch 27/30\n",
      "91074/91074 [==============================] - 1s 7us/sample - loss: 8.3199\n",
      "Epoch 28/30\n",
      "91074/91074 [==============================] - 1s 8us/sample - loss: 8.3080\n",
      "Epoch 29/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 8.3138\n",
      "Epoch 30/30\n",
      "91074/91074 [==============================] - 1s 9us/sample - loss: 8.3095\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "vae_model.compile(optimizer=\"rmsprop\", loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "# Finally, we train the model:\n",
    "results = vae_model.fit(train_2_scaled, train_2_scaled,\n",
    "                        shuffle=True,\n",
    "                        epochs=30,\n",
    "                        batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss 그래프로 모델 학습 결과 확인\n",
    "\n",
    "plt.plot(results.history['loss'])\n",
    "#plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg error 0.024591961732877945\n",
      "median error 0.022823635358577607\n",
      "99Q: 0.060008623615320646\n",
      "setting threshold on 0.060008623615320646 \n"
     ]
    }
   ],
   "source": [
    "# 예측 및 문턱값 정의\n",
    "\n",
    "val_2_pred = vae_model.predict(val_2_scaled)\n",
    "\n",
    "mae_vector = get_error_term(val_2_pred, val_2_scaled, _rmse=False)\n",
    "\n",
    "print(f'Avg error {np.mean(mae_vector)}\\nmedian error {np.median(mae_vector)}\\n99Q: {np.quantile(mae_vector, 0.99)}')\n",
    "print(f'setting threshold on { np.quantile(mae_vector, 0.99)} ')\n",
    "\n",
    "error_thresh = np.quantile(mae_vector, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010013351134846462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이상치 정의 및 비율 파악\n",
    "anomalies = (mae_vector > error_thresh)\n",
    "np.count_nonzero(anomalies) / len(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score : [0.5738321939385375]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     28432\n",
      "           1       0.08      0.80      0.15        30\n",
      "\n",
      "    accuracy                           0.99     28462\n",
      "   macro avg       0.54      0.90      0.57     28462\n",
      "weighted avg       1.00      0.99      0.99     28462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "# from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "val_score_2 = f1_score(val_y, anomalies, average='macro')\n",
    "\n",
    "print(f'Validation F1 Score : [{val_score_2}]')\n",
    "print(classification_report(val_y, anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 전체 특성 사용 ============\n",
      "Validation F1 Score : [0.5706399257283392]\n",
      "============== \bPCA 진행 ==============\n",
      "Validation F1 Score : [0.5706399257283392]\n",
      "============ 특성 임의 선택 ============\n",
      "Validation F1 Score : [0.5738321939385375]\n"
     ]
    }
   ],
   "source": [
    "# 세 모델 스코어 정리\n",
    "print(\"============ 전체 특성 사용 ============\")\n",
    "print(f'Validation F1 Score : [{val_score}]')\n",
    "print(\"============== PCA 진행 ==============\")\n",
    "print(f'Validation F1 Score : [{val_score_pca}]')\n",
    "print(\"============ 특성 임의 선택 ============\")\n",
    "print(f'Validation F1 Score : [{val_score_2}]')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
